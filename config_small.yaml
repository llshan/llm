experiment_name: "small_toy_run"

model_size: "small"          # 切换为 small
tokenizer_name: "gpt2"

data:
  train_path: "data/wiki_toy_train.jsonl"
  val_path: "data/wiki_toy_val.jsonl"
  block_size: 128            # 增加 block_size 以利用 small 模型的能力 (最大支持 256)
  train_batch_size: 2        # 保持较小，防止 OOM
  val_batch_size: 2
  num_workers: 0

training:
  max_epochs: 2
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_steps: 10
  accelerator: "mps"
  devices: 1
  precision: 32
  log_every_n_steps: 1
  default_root_dir: "outputs"
