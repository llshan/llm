experiment_name: "tiny_toy_run"

model_size: "tiny"          # 本地先跑 tiny，确认没问题再换 small
tokenizer_name: "gpt2"

data:
  train_path: "data/processed_medium/train.jsonl"  # 使用更大的数据集
  val_path: "data/processed_medium/val.jsonl"
  block_size: 128            # 增加上下文长度
  train_batch_size: 8        # M1/M2 跑 tiny 模型可以适当加大 batch
  val_batch_size: 8
  num_workers: 4             # 消除警告，加速加载

training:
  max_epochs: 5              # 多跑几轮，看 loss 下降
  learning_rate: 5e-4        # Tiny 模型可以用稍大的学习率
  weight_decay: 0.01
  warmup_steps: 10
  accelerator: "mps"
  devices: 1
  precision: 32
  log_every_n_steps: 1
  default_root_dir: "outputs"

